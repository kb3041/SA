# Default LR was 0.0001.
# Set LR = 0.000075 but doesn't seem to affect the training.
# Set LR = 0.001 and the results were shit.

# Set embedding size == 350. Results worse than pre-trained w2v.
# Set embedding size == 275. Results even worse.
# Set embedding size == 225. Yep, even worse.

 * Embedding size == 350
    * 3,4,5,7 filter sizes and 256 of each. (0.85580)
 * Keep embeddings at 300 or 350 and use larger filter size.
    * Doesn't help almost at all.
    * Perhaps could be tried once we get a really better model.

Fixed w2v bug (18 Jun)

 * Embedding size == 300
   * 1st stage of Nikos's preprocessing (0.85840)
        - used local-trained 300d word2vec
   * 2nd stage of Nikos's preprocessing (0.86160)
        - used pre-computed embeddings by design
        - increased max sentence length to 35 instead of 30

 * Nikos's preprocessing (es = 300) + LSTM
   * 1st (killed off) Euler run
        - 24h for 6 epochs. Still far from converged, it seems.
        - Added dropout just to be sure.
        - Dev score:    <see TensorBoard; slightly higher than best CNN>
        - Kaggle score: <data was bad>
   * 2nd Euler run
        - running; max 48h
        - Kaggle score after 68k steps: 0.86920

 * CNN + Nikos's preprocessing:
   * LOW: 4, HI: 20, HTFB: 10, 96k steps: 0.86680
   * LOW: 4, HI: 20, HTFB: 10, 68k steps: 0.86540

 * Deep LSTM + Nikos's preprocessing (no optimal params):
   * Underfit.

